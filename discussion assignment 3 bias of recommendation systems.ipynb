{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIAS OF RECOMMENDATION SYSTEM\n",
    "\n",
    "            Recommender systems have been applied successfully in a number of different domains, such as, entertainment, commerce, and employment.  Their success relies on their capability to exploit the collective behavior of users in order to deliver highly targeted, personalized recommendations. Given that recommenders learn from user preferences, they also incorporate different biases that users exhibit in the input data.  For example, it is natural to expect gender bias when recommending clothes. However, gender bias is undesirable when recommending job postings, or information content. Furthermore, we want to avoid the case where the recommender system introduces bias in the data, by amplifying existing biases and reinforcing stereotypes. We refer to this phenomenon, where input and recommendation bias differ, as bias disparity.\n",
    "\n",
    "             Bias disparity is widely happened in recommendation systems.  One of the most popular classes of recommendation systems is collaborative filtering. Collaborative Filtering (CF) uses the collective behavior of all users over all items to infer the preferences of individual users for specific items. However, given the reliance of CF algorithms on the input preferences, they are susceptible to biases that may appear in the input data. In this work, we consider biases with respect to the preferences of specific groups of users (e.g., men and women) towards specific categories of items (e.g., different movie genres).  Bias in recommendations is not necessarily always problematic.  The problem of algorithmic bias, and its flip side, fairness in algorithms, has attracted considerable attention in the recent years. Most existing work focuses on classification systems, while there is limited work on recommendation systems. One type of recommendation bias that has been considered in the literature is popularity bias. It has been observed that under some conditions popular items are more likely to be recommended leading to a rich get richer effect, and there are some attempts to correct this bias. Related to this is also the quest for diversity, where the goal is to include different types of items in the recommendations.\n",
    "\n",
    "             In the past few years, researchers, designers, developers and legal scholars have begun to develop new methods for preventing, identifying, and addressing bias in recommender systems and other machine learning applications, and to formulate best practices for organizations that engage in machine learning development. There are a couple of ways to correct the bias in recommend system.  For example, approach is based on a computational post-hoc de-biasing algorithm that systematically adjusts the user-submitted ratings that are known to be biased. Approach is a user-interface-driven solution that tries to minimize anchoring biases at rating collection time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
